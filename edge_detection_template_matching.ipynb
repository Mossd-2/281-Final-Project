{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Matching and Edge Detection Feature Sets\n",
    "\n",
    "This notebook includes functions and the pipeline for pulling edges and template matching statistics as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:35:05.148491: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-24 17:35:05.166342: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-24 17:35:05.188075: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-24 17:35:05.192597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-24 17:35:05.202714: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-24 17:35:05.790259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
    "import cv2\n",
    "from glob import glob\n",
    "import utils\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = None  # Will be set by setup_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data():\n",
    "    \"\"\"Set up the data path based on environment.\"\"\"\n",
    "    global DATA_PATH\n",
    "    \n",
    "    # Check if we're in Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "        \n",
    "        # Mount Drive if not already mounted\n",
    "        if not os.path.exists('/content/drive'):\n",
    "            drive.mount('/content/drive')\n",
    "        \n",
    "        # Ask for path to project folder\n",
    "        print(\"Enter path to the 281_final_project_data folder in Drive (e.g., /content/drive/MyDrive/281_final_project_data):\")\n",
    "        project_path = input()\n",
    "        \n",
    "        # Set the data path to the basic folder within the project\n",
    "        DATA_PATH = os.path.join(project_path, \"basic\")\n",
    "            \n",
    "    except ImportError:\n",
    "        # Not in Colab, download the data if needed\n",
    "        download_path = \"./281_final_project_data\"\n",
    "        \n",
    "        # Check if data already exists\n",
    "        if os.path.exists(download_path) and os.path.exists(os.path.join(download_path, \"basic\")):\n",
    "            print(f\"Using existing data in {download_path}/basic\")\n",
    "        else:\n",
    "            print(\"Download data 281_final_project_data folder from Google Drive.\")\n",
    "        \n",
    "        # Set the data path to the basic folder within the downloaded project\n",
    "        DATA_PATH = os.path.join(download_path, \"basic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images function\n",
    "def load_data(directory):\n",
    "    \n",
    "    # First, ensure the data is available\n",
    "    setup_data()\n",
    "    path_to_data = DATA_PATH\n",
    "    \n",
    "    # load text file with image labels as a dictionary\n",
    "    labels = pd.read_csv(os.path.join(path_to_data, \"EmoLabel/list_patition_label.txt\"), sep=\" \", header=None)\n",
    "    labels = dict(zip(labels[0], labels[1]))\n",
    "    \n",
    "    # update path_to_data\n",
    "    path_to_data = os.path.join(path_to_data, \"Image\", directory)\n",
    "\n",
    "    train_img = []\n",
    "    train_labels = []\n",
    "    test_img = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for file in os.listdir(path_to_data):\n",
    "        image_path = os.path.join(path_to_data, file)\n",
    "        image = load_img(image_path)\n",
    "        img_arr = img_to_array(image, dtype=int)\n",
    "        if directory == \"aligned\":\n",
    "            label = labels[file.replace(\"_aligned\", \"\")]\n",
    "        else:\n",
    "            label = labels[file]\n",
    "        if \"train\" in file:\n",
    "            train_img.append(img_arr)\n",
    "            train_labels.append(label)\n",
    "        else:\n",
    "            test_img.append(img_arr)\n",
    "            test_labels.append(label)\n",
    "\n",
    "    train_labels = np.array(train_labels, dtype=int)\n",
    "    test_labels = np.array(test_labels, dtype=int)\n",
    "    \n",
    "    if directory == \"aligned\":\n",
    "        train_img = np.array(train_img)\n",
    "        test_img = np.array(test_img)\n",
    "\n",
    "        # Apply random shuffling to training examples.\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(train_img.shape[0])\n",
    "        shuffled_indices = np.random.permutation(indices)\n",
    "        train_img = train_img[shuffled_indices]\n",
    "        train_labels = train_labels[shuffled_indices]\n",
    "\n",
    "    return train_img, train_labels, test_img, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label names\n",
    "label_names_dict = {1: 'surprise', 2: 'fear', 3: 'disgust', 4: 'happiness', 5: 'sadness', 6: 'anger', 7: 'neutral'}\n",
    "\n",
    "# Images are loaded as ints from 0 to 255\n",
    "# Load training images and labels (original)\n",
    "# Note: Loads images as a regular list since images are all different sizes. Labels are numpy array. \n",
    "# If loading originals, they will require further processing and shuffling. \n",
    "#train_img_org, train_labels_org, test_img_org, test_labels_org = load_data(DATA_PATH, \"original\")\n",
    "\n",
    "# Load training images and labels (aligned)\n",
    "# Converts images to numpy array since images are 100x100 and shuffles them. Labels are numpy array. \n",
    "train_img_aligned, train_labels_aligned, test_img_aligned, test_labels_aligned = load_data(\"aligned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Canny Edges to the Images\n",
    "\n",
    "Canny edges can be added as a channel into the images (shape will then become (H, W, 4) instead of (H, W, 3)) and added as an input into the model, or be used in a pipeline to identify contours of facial landmarks (such as eyes, mouth, nose, etc), or sillhouettes, which can be included as feature statistics into the model.\n",
    "\n",
    "The below function adds the canny filters into the image as an extra channel and outputs the manipulated image into a designated output folder. we can adjust this to output each image individually or some other format for our pipeline, depending on how it fits with the other feature extraction steps. \n",
    "\n",
    "### Template Matching statistics as Selected Features\n",
    "One way to use template matching for feature engineering would be to have templates for each facial expression (e.g. raised eyebrows and open mouth for suprise, smile and thinner eyes for happy, etc). The below function gathers similarity metrics between an image and a template and could be implemented into a pipeline for us to gather these summary statistics across all facial expression templates. E.g. min_val, max_val, avg_score, std_score will be gathered for each facial expression and included in our feature set. \n",
    "\n",
    "One way of getting these templates is to use the training images and get an average face for each emotion class, then apply the canny detection pipeline to each template. This may add more information than a simple smiley face or frowning face. But if we do this should pay close attention to the test statistics as the templates could be overfit to the training data. We should also inspect those average faces to see if this seems feasible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# processing all images and outputing list of canny edges\n",
    "def process_images_with_canny(images, lower_threshold = 50, upper_threshold = 150): #suggested thresholds for detecting edges with canny\n",
    "    \"\"\" Converts images to grayscale, enhances contrast, and applies Canny edge detection. outputs list of np.arrays\"\"\"\n",
    "\n",
    "    processed_images = []\n",
    "       \n",
    "    for img in images:\n",
    "        # grayscaling\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # equalizing histogram (contrast)\n",
    "        img_eq = cv2.equalizeHist(img)\n",
    "        \n",
    "        # applying canny edge detection\n",
    "        edges = cv2.Canny(img_eq, lower_threshold, upper_threshold)\n",
    "        \n",
    "        # appending to output list\n",
    "        processed_images.append(edges)\n",
    "\n",
    "    return processed_images\n",
    " \n",
    " \n",
    " \n",
    "# getting an average face for each expression from our training data and applying canny filter \n",
    "\n",
    "def create_average_template(images, labels):\n",
    "    \"\"\"Creates an average face for all images for each emotion class, then outputting into a dict.\"\"\"\n",
    "\n",
    "    emotion_dict = {}\n",
    "    unique_labels = set(labels)\n",
    "\n",
    "    for lab in unique_labels:\n",
    "        # Select all images with the current label\n",
    "        float_images = [\n",
    "            cv2.equalizeHist(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)).astype(np.float32)\n",
    "            for img, label in zip(images, labels) if label == lab\n",
    "        ]\n",
    "    \n",
    "        #getting average image of list\n",
    "        avg_img = np.mean(float_images, axis=0)\n",
    "    \n",
    "        #applying canny filter\n",
    "        avg_img = cv2.Canny(avg_img, 50, 150)\n",
    "        \n",
    "        # appending to dict\n",
    "        emotion_dict[lab] = avg_img        \n",
    "        \n",
    "\n",
    "    return emotion_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# getting similarity statistics and outputing a vector of features for each emotion template \n",
    "def extract_expression_match_features_batch(images, labels, emotion_templates):\n",
    "    \"\"\"\n",
    "    Takes a list of images and labels, and a dict of preprocessed emotion templates (as np.arrays).\n",
    "    Returns:\n",
    "        - A list of feature vectors (one per image).\n",
    "        - A list of corresponding labels.\n",
    "    \"\"\"\n",
    "    feature_matrix = []\n",
    "    output_labels = []\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "        # grayscale and equalizing imgs\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.equalizeHist(img)\n",
    "        #applying canny edges\n",
    "        img_edges = cv2.Canny(img, 50, 150)\n",
    "\n",
    "        feature_vector = []\n",
    "\n",
    "        #getting similarity metrics for each emotion template and adding as features \n",
    "        for emotion, template in emotion_templates.items():\n",
    "\n",
    "            result = cv2.matchTemplate(img_edges, template, cv2.TM_CCOEFF_NORMED)\n",
    "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "            avg_score = np.mean(result)\n",
    "            std_score = np.std(result)\n",
    "\n",
    "            feature_vector.extend([max_val, avg_score, std_score])\n",
    "\n",
    "        feature_matrix.append(np.array(feature_vector))\n",
    "        output_labels.append(label)\n",
    "\n",
    "    return feature_matrix, output_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Functions to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing and outputing edges vector \n",
    "edges_features_2d = process_images_with_canny(train_img_aligned)\n",
    "edges_features_1d = edges_features_2d.reshape(train_img_aligned.shape[0], -1) # flattening to just 1d feature vector for each image \n",
    "\n",
    "\n",
    "# creating emotion templates\n",
    "templates_dict = create_average_template(train_img_aligned, train_labels_aligned)\n",
    "\n",
    "\n",
    "# gathering similarity metrics to add to the feature set\n",
    "template_match_features = extract_expression_match_features_batch(train_img_aligned, train_labels_aligned, templates_dict)\n",
    "\n",
    "\n",
    "# final edges and template matching feature set\n",
    "edge_templ_features = np.concatenate([edges_features_1d, template_match_features], axis=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
